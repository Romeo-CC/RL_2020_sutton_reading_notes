{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/imgs/Page1.jpg\" alt=\"RL basic idea\" width=\"500\" height=\"333\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of MDP is a conditional probability as:\n",
    "\n",
    "$$\n",
    "p(s^{'}, r|s, a) \\doteq \\Pr\\{S_t = s^{'}, R_t=r|S_{t-1} = s, A_{t-1} = a\\}\n",
    "$$\n",
    "\n",
    "for all $s, s^{'} \\in \\mathcal{S} $, $r \\in \\mathcal{R}$, $a\\in \\mathcal{A}(s)$\n",
    "\n",
    "$p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A}  \\rightarrow [0, 1]$\n",
    "\n",
    "$$\n",
    "\\sum_{s^{'}\\in\\mathcal{S}}\\sum_{r \\in \\mathcal{R}} p(s^{'}, r|s, a) = 1, \\text{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "$$\n",
    "\n",
    "he probability of each possible value for $S_t$ and $R_t$ depends on the ***immediately preceding state and action***, $S_{t-1}$ and $A_{t-1}$, and, given them, ***not at all on earlier states and actions***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Markov property***\n",
    "\n",
    "+ The future is independent of the past given the present.\n",
    "\n",
    "What this means is that, to predict the next state and reward, you only need to know the current state. You don't need to know the entire history of previous states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formal Definition\n",
    "\n",
    "Mathematically, a state $S_t$ has the Markov property if:\n",
    "$$\n",
    "P(S_{t+1}∣S_t)=P(S_{t+1}∣S_1, S_2, ..., S_t)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(R_{t+1}∣S_t)=P(R_{t+1}∣S_1, S_2, ..., S_t)\n",
    "$$\n",
    "Where:\n",
    "+ $S_t$is the state at time $t$.\n",
    "+ $R_{t+1}$ is the reward at time $t+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***State-transition probabilities***\n",
    "\n",
    "$$\n",
    "p(s^{'} |s, a) \\doteq \\Pr\\{S_t = s^{'}|S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in \\mathcal{R}}p(s^{'}, r|s, a)\n",
    "$$\n",
    "\n",
    "$p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A}  \\rightarrow [0, 1] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected rewards for state–action pairs\n",
    "$$\n",
    "r(s, a) \\doteq \\mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s^{'}\\in\\mathcal{S}} p(s^{'}, r|s, a)\n",
    "$$\n",
    "$r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected rewards for state–action–next-state triples\n",
    "\n",
    "$$\n",
    "r(s, a, s^{'}) \\doteq \\mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a, S_t=s^{'}] = \\sum_{r\\in\\mathcal{R}} r \\frac{p(s^{'}, r|s, a)}{p(s^{'} |s, a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.4\n",
    "\n",
    "\n",
    "| $s$ | $a$ | $s^{'}$ | $r$ | $p(s^{'}, r\\|s, a)$ |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| high | search | high | $r_{search}$ | $\\alpha$ |\n",
    "| high | search | low | $r_{search}$ | $1 - \\alpha$ |\n",
    "| high | wait | high | $r_{wait}$ | 1 |\n",
    "| high | wait | low | - | 0 |\n",
    "| low | recharge | high | 0 | 1 |\n",
    "| low | recharge | low | - | 0 |\n",
    "| low | wait | high | - | 0 |\n",
    "| low | wait | low | $r_{wait}$ | 1 |\n",
    "| low | search | high | -3 | $1 - \\beta $ |\n",
    "| low | search | low | $r_{search}$ | $\\beta$ | \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
