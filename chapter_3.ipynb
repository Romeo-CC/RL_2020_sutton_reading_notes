{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/imgs/Page1.jpg\" alt=\"RL basic idea\" width=\"500\" height=\"333\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of MDP is a conditional probability as:\n",
    "\n",
    "$$\n",
    "p(s', r|s, a) \\doteq \\Pr\\{S_t = s', R_t=r|S_{t-1} = s, A_{t-1} = a\\}\n",
    "$$\n",
    "\n",
    "for all $s, s' \\in \\mathcal{S} $, $r \\in \\mathcal{R}$, $a\\in \\mathcal{A}(s)$\n",
    "\n",
    "$p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A}  \\rightarrow [0, 1]$\n",
    "\n",
    "$$\n",
    "\\sum_{s'\\in\\mathcal{S}}\\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1, \\text{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "$$\n",
    "\n",
    "The probability of each possible value for $S_t$ and $R_t$ depends on the ***immediately preceding state and action***, $S_{t-1}$ and $A_{t-1}$, and, given them, ***not at all on earlier states and actions***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Markov property***\n",
    "\n",
    "+ The future is independent of the past given the present.\n",
    "\n",
    "What this means is that, to predict the next state and reward, you only need to know the current state. You don't need to know the entire history of previous states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formal Definition\n",
    "\n",
    "Mathematically, a state $S_t$ has the Markov property if:\n",
    "$$\n",
    "P(S_{t+1}∣S_t)=P(S_{t+1}∣S_1, S_2, ..., S_t)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(R_{t+1}∣S_t)=P(R_{t+1}∣S_1, S_2, ..., S_t)\n",
    "$$\n",
    "Where:\n",
    "+ $S_t$is the state at time $t$.\n",
    "+ $R_{t+1}$ is the reward at time $t+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***State-transition probabilities***\n",
    "\n",
    "$$\n",
    "p(s' |s, a) \\doteq \\Pr\\{S_t = s'|S_{t-1} = s, A_{t-1} = a\\} = \\sum_{r \\in \\mathcal{R}}p(s', r|s, a)\n",
    "$$\n",
    "\n",
    "$p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A}  \\rightarrow [0, 1] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected rewards for state–action pairs\n",
    "$$\n",
    "r(s, a) \\doteq \\mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s', r|s, a)\n",
    "$$\n",
    "$r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected rewards for state–action–next-state triples\n",
    "\n",
    "$$\n",
    "r(s, a, s') \\doteq \\mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a, S_t=s'] = \\sum_{r\\in\\mathcal{R}} r \\frac{p(s', r|s, a)}{p(s' |s, a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.4\n",
    "\n",
    "\n",
    "| $s$ | $a$ | $s'$ | $r$ | $p(s', r\\|s, a)$ |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| high | search | high | $r_{search}$ | $\\alpha$ |\n",
    "| high | search | low | $r_{search}$ | $1 - \\alpha$ |\n",
    "| high | wait | high | $r_{wait}$ | 1 |\n",
    "| high | wait | low | - | 0 |\n",
    "| low | recharge | high | 0 | 1 |\n",
    "| low | recharge | low | - | 0 |\n",
    "| low | wait | high | - | 0 |\n",
    "| low | wait | low | $r_{wait}$ | 1 |\n",
    "| low | search | high | -3 | $1 - \\beta $ |\n",
    "| low | search | low | $r_{search}$ | $\\beta$ | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Goals and Rewards\n",
    "\n",
    "The agent’s goal is to maximizethe total amount of reward it receives. This means maximizing **not immediate reward**, but **cumulative reward in the long run**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reward hypothesis*\n",
    "\n",
    "All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.\n",
    "\n",
    "Better places for imparting this kind of prior knowledge are the initial policy or initial value function.\n",
    "\n",
    "The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Returns and Episodes\n",
    "\n",
    "Episode: a **independent** trial **ends with the terminal state**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... \\\\\n",
    "\\newline\n",
    "&= \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1} \\\\\n",
    "\\newline\n",
    "&= R_{t+1} + \\gamma\\Big(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ...\\Big) \\\\\n",
    "\\newline\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\\\\n",
    "\\newline\n",
    "&= \\sum_{k=t+1}^{T}\\gamma^{k - t - 1}R_{k} \\\\                       \n",
    "\\text{Unified Notation}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5  Policies and Value Functions\n",
    "\n",
    "#### Policy\n",
    "Policy: a mapping from states to probabilities of selecting each possible action.\n",
    " \n",
    "$\\pi(a|s)$ defines a probability distribution over $a\\in\\mathcal{A}(s)$ for each $s\\in\\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Exercise 3.11\n",
    "\n",
    "If the current state is $S_t$, and actions are selected according to a stochastic policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function \n",
    "$$\n",
    "p(s', r|s, a) \\doteq \\Pr\\{S_t = s', R_t=r|S_{t-1} = s, A_{t-1} = a\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Given current $S_t$ calculate $\\mathbb{E}(R_{t+1}|S_t=s)$\n",
    "\n",
    "Using Law of Total Expectation, the value of $R_{t+1}$ depends on the action $A_t$ taken in state $s = S_t$ and the subsequent transition determined by the environment dynamics $p(s', r|s, a)$ We can condition on the action $A_t=a$ :\n",
    "$$\n",
    "\\mathbb{E}_\\pi[R_{t+1} | S_t = s] = \\sum_{a \\in \\mathcal{A}(s)} \\Pr\\{A_t = a | S_t = s\\} \\cdot \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]\n",
    "$$\n",
    "where $\\mathcal{A}(s)$ is the set of possible actions in state s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy \n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\Pr\\{A_t = a | S_t = s\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "substitute into above equation\n",
    "$$\n",
    "\\mathbb{E}_\\pi[R_{t+1} | S_t = s] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\cdot \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are in state $s$ and have taken action $a$. This expectation is determined by the environment's dynamics $p(s', r | s, a)$, which gives the probability of transitioning to state $s'$ and receiving reward $r$. To get the expected reward, we sum over all possible next states $s'$ and all possible rewards $r$, weighting each reward $r$ by its probability of occurring:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r | s, a) \\cdot r\n",
    "$$\n",
    "\n",
    "where $\\mathcal{S}$ represents the set of states, $\\mathcal{R}$ represents the set of rewards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "substitute back into the main equation, we get \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb{E}_\\pi[R_{t+1} | S_t = s] &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(a|s) \\left( \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r | s, a) \\cdot r \\right) \\\\\n",
    "\\newline\n",
    "&= \\sum_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} \\pi(a|s) \\, p(s', r | s, a) \\, r\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value function of a state $s$ for MDPs is the expected return starting from state $s$ and then following policy $\\pi$, called state-value function for policy $\\pi$\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}\\big[G_t|S_t=s\\big] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\Bigg|S_t=s\\Bigg]  \n",
    "$$\n",
    "for all $s \\in \\mathcal{S}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_{\\pi}(s, a)$, is the expected return starting from $s$, taking the action $a$, following pocily $\\pi$, called action-value function for policy $\\pi$\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\big[G_t|S_t=s, A_t=a\\big] = \\mathbb{E}_{\\pi}\\Bigg[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\Bigg|S_t=s, A_t=a\\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.12 Give an equation for $v_{\\pi}$ in terms of $q_{\\pi}$ and $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(s)$ is the state-value function. This function represents the expected total discounted future reward an agent can get starting from state $s$ and then following the policy $\\pi$ for all future decisions. It tells you how good it is to be in state $s$ under policy $\\pi$.\n",
    "\n",
    "$q_{\\pi}(s, a)$ is the action-value function. This function represents the expected total discounted future reward an agent can get starting from state $s$, taking a specific action $a$, and then following the policy $\\pi$ for all subsequent decisions. It tells you how good it is to take action $a$ in state $s$ (and then follow $\\pi$). \n",
    "\n",
    "policy $\\pi(a|s)$ is the strategy agent follows to give a specific action $a$ for any given state $s$, which is a probability distribution\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A(s)}} \\pi(a|s) q_{\\pi}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.13 Give an equation for $q_{\\pi}$ in terms of $v_{\\pi}$ and the four-argument $p(s', r | s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamics Function  $p(s', r|s, a) \\doteq \\Pr\\{S_t = s', R_t=r|S_{t-1} = s, A_{t-1} = a\\}$ gives the probability of transitioning to the next state $s'$ and receiving reward $r$, given that the agent was in state $s$ and took action $a$.\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\big[G_t|S_t=s, A_t=a\\big] \n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{split}\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... \\\\\n",
    "\\newline\n",
    "&= R_{t+1} + \\gamma\\Big(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ...\\Big) \\\\\n",
    "\\newline\n",
    "&= R_{t+1} + \\gamma G_{t+1} \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute $G_t$ into $q_{\\pi}(s, a)$\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\bigg[R_{t+1} + \\gamma G_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "$$\n",
    "\n",
    "using linearity feature of Expectation\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\bigg[R_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "+ \\gamma \\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First item is the expected value of the reward $R_{t+1}$ you receive immediately after taking action $a$ in state $s$. To calculate this, we sum over all possible next states $s'$ and rewards $r$, weighted by their joint probability $p(s',r∣s,a)$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\bigg[R_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second item represents the discounted value of starting from the state $S_{t+1}$ you land in after taking action $a$. \n",
    "\n",
    "The expectation $\\mathbb{E}_{\\pi}[G_{t+1}|S_t=s, A_t=a]$ depends on the distribution of possible next states $S_{t+1}$. \n",
    "\n",
    "If the process transitions to a specific next state $S_{t+1}=s'$, the expected return from that point onwards, following policy $\\pi$, is precisely the definition of the state-value function $v_{\\pi}(s')$.\n",
    "\n",
    "That is, \n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\big[G_{t+1}|S_{t+1}=s'\\big] = v_{\\pi}(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to average the values $v_{\\pi}(s')$ over all possible next states $s'$, weighted by their transition probabilities. \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\big[G_{t+1}|S_t=s, A_t=a\\big] \n",
    "\n",
    "= \\sum_{s' \\in \\mathcal(S)} \\Pr\\{S_{t+1}=s'|S_t=s, A_t=a\\} \n",
    "\n",
    "\\cdot \\mathbb{E}_{\\pi}\\big[G_{t+1}|S_{t+1}=s'\\big]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The probability of transitioning to state $s'$ (regardless of the reward received) given $s$ and $a$ is \n",
    "$$\n",
    "p(s'|s, a) = \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\big[G_{t+1}|S_t=s, A_t=a\\big] \n",
    "\n",
    "= \\sum_{s' \\in \\mathcal(S)} \\Bigg(\\sum_{r \\in \\mathcal{R}} p(s',r∣s,a)\\Bigg) v_{\\pi}(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "q_{\\pi}(s, a) &= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) r\n",
    "+ \\gamma \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\cdot v_{\\pi}(s') \\\\\n",
    "\\newline\n",
    "&= \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\big[ r + \\gamma v_{\\pi}(s') \\big]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation states that the value of taking action $a$ in state $s$ (and then following $\\pi$) is the expected value of the sum of the immediate reward $r$ and the discounted value of the next state $s'$. \n",
    "\n",
    "The expectation is taken over all possible next states $s'$ and rewards $r$, according to the dynamics $p(s',r∣s,a)$ determined by the environment given the current state $s$ and the chosen action $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation for $v_{\\pi}$\n",
    "\n",
    "combine Exercise3.12 and 3.13\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A(s)}} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r\\in \\mathcal{R}} p(s', r|s, a) \\bigg[r + \\gamma v_{\\pi}(s')\\bigg]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman equation for $v_{\\pi}$ expresses the value of a state $v_{\\pi}(s)$ recursively in terms of the values of its potential successor states $v_{\\pi}(s')$. \n",
    "\n",
    "+ The value of being in state $s$ under policy $\\pi$, denoted $v_{\\pi}(s)$ ,is equal to the expectation over:\n",
    "    + Actions $a$ taken according to the policy $\\pi(a∣s)$.\n",
    "    + Next states $s'$ and rewards $r$ resulting from the environment dynamics $p(s',r∣s,a)$.\n",
    "+ of the sum of:\n",
    "    + The immediate reward $r$.\n",
    "    + The discounted value $\\gamma$ of the next state $v_{\\pi}(s')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/imgs/bellman.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why is it important?\n",
    "\n",
    "1. Recursive Relationship: It defines the value of a state in terms of the values of potential next states. This recursive structure is key.\n",
    "\n",
    "2. Consistency Condition: It expresses a condition that the true value function $v_{\\pi}$ must satisfy. If you have the correct $v_{\\pi}$, this equation will hold true for all states $s$.\n",
    "\n",
    "3. Basis for Algorithms: This equation is the foundation for algorithms that compute the value function for a given policy (a process called policy evaluation). Iterative methods can be used to find the value function $v_{\\pi}$ that satisfies this equation for all states simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Exercise 3.14\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A(s)}} \\pi(a|s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r\\in \\mathcal{R}} p(s', r|s, a) \\bigg[r + \\gamma v_{\\pi}(s')\\bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "v_{center} = \\frac{1}{4} \\times 0.9 \\times (v_{up} + v_{down} + v_{left} + v_{right}) = 0.675 \\approx 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.15\n",
    "\n",
    "fomula 3.8\n",
    "$$\n",
    "\\begin{split}\n",
    "G_t &\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... \\\\\n",
    "\\newline\n",
    "&= \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding constant $c$ to all the rewards\n",
    "$$\n",
    "\\begin{split}\n",
    "& (R_{t+1} + c) + \\gamma (R_{t+2}+c) + \\gamma^2 (R_{t+3}+c) + \\gamma^3 (R_{t+4}+c) + ... \\\\\n",
    "\\newline\n",
    "=& \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1} + (c + \\gamma c + \\gamma^2 c + \\gamma^3 c + ...) \\\\\n",
    "\\newline\n",
    "=& \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1} + \\sum_{k=0}^{\\infty}c\\gamma^{k} \\\\\n",
    "\\newline\n",
    "=& G_t + \\lim_{n\\rightarrow \\infty} \\frac{c(1 - \\gamma^{n})}{1-\\gamma} \\\\\n",
    "\\newline\n",
    "=& G_t + \\frac{c}{1-\\gamma}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.17\n",
    "\n",
    "#### Bellman Equation for action-value function $q_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from exercise 3.13 \n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\bigg[R_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "+ \\gamma \\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First item is the expected value of the reward $R_{t+1}$ you receive immediately after taking action $a$ in state $s$. To calculate this, we sum over all possible next states $s'$ and rewards $r$, weighted by their joint probability $p(s',r∣s,a)$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\bigg[R_{t+1}\\bigg|S_t=s, A_t=a \\bigg]\n",
    "= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg]$ represents the expected return from the next time step onward, given we arrived there via $(s,a)$. \n",
    "\n",
    "This expectation depends on the state $S_{t+1}=s'$ we land in and the action $A_{t+1}=a'$ chosen by the policy $\\pi$ in that state $s'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected return after landing in $s'$ and choosing $a'$ according to $\\pi(a'|s')$ is $q_{\\pi}(s', a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we first need to find the expected value starting from state $s'$,\n",
    "$$\n",
    "\\begin{split}\n",
    "v_{\\pi}(s') &= \\mathbb{E}_{\\pi}\\big[G_{t+1}|S_{t+1}=s'\\big] \\\\\n",
    "\\newline\n",
    " &= \\sum_{a' \\in \\mathcal{A}(s')}\\pi(a'|s') \\cdot q_{\\pi}(s', a')\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall expectation $\\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg]$ is found by averaging $v_{\\pi}(s')$ over the possible next states $s'$ resulting from (s,a):\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg] = \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\cdot v_{\\pi}(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "substitute $v_{\\pi}(s')$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi}\\bigg[G_{t+1}\\bigg|S_t=s, A_t=a \\bigg] = \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\cdot \\Bigg( \\sum_{a' \\in \\mathcal{A}(s')}\\pi(a'|s') \\cdot q_{\\pi}(s', a') \\Bigg)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "q_{\\pi}(s, a) &= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) r\n",
    "+ \\gamma \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\cdot \\Bigg( \\sum_{a' \\in \\mathcal{A}(s')}\\pi(a'|s') \\cdot q_{\\pi}(s', a') \\Bigg)\\\\\n",
    "\\newline\n",
    "&= \\sum_{s' \\in \\mathcal(S)} \\sum_{r \\in \\mathcal{R}} p(s',r∣s,a) \\Bigg[r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')}\\pi(a'|s') \\cdot q_{\\pi}(s', a') \\Bigg]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation states that:\n",
    "\n",
    "+ The value of taking action $a$ in state $s$ and then following policy $\\pi$, denoted $q_{\\pi}(s, a)$\n",
    "+ is equal to the expectation over:\n",
    "  + Next states $s'$ and rewards $r$ resulting from the environment dynamics $p(s',r∣s,a)$ after taking action $a$ in state $s$.\n",
    "\n",
    "+ of the sum of:\n",
    "  + The immediate reward $r$.\n",
    "  + The discounted value ($\\gamma$) of the next state-action pair. The value of the next state $s'$ is itself an expectation over the actions $a'$ chosen by the policy $\\pi$ in that state $\\sum_{a' \\in \\mathcal{A}(s')}\\pi(a'|s') \\cdot q_{\\pi}(s', a')$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
