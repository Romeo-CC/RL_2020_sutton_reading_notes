{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/imgs/dl/Page1.jpg\" alt=\"RL basic idea\" width=\"500\" height=\"333\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Learning from interaction\n",
    "map situations to actions \n",
    "trial and error search \n",
    "delayed reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "...ing, like machine learning, is simultaneously a problem, a class of solution methods that work \n",
    "well on the problem, and the field studies this problem and its solution methods.\n",
    "\n",
    "It is convenient to use single name for all three things, but at the same time essential to keep\n",
    "the three conceptually separate.\n",
    "\n",
    ". In particular, the distinction between problems and solution methods is very important in \n",
    "reinforcement learning; failing tomake this distinction is the source of many confusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the real problem facing a learningagent interacting over time with its environment to achieve a goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning agent must be able to **`sense`** the state of its environment to some extent and must be able to take **`actions`** that affect the state.\n",
    "\n",
    "The agent also must have a **`goal`** or goals relating to the state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of this kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly in situations not present in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In interactive problems it is often impractical to obtain examples of desired behavior that are both correct and representative of all thesituations in which the agent has to act."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior,reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the third paradigm of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Reinforcement Learning\n",
    "\n",
    "1. trade-off between exploration and exploitation\n",
    " <!-- exploitation -->\n",
    " To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward.\n",
    " <!-- exploration -->\n",
    " But to discover such actions, it has to try actions that it has not selected before.\n",
    " \n",
    " The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future.\n",
    "\n",
    " The dilemma is that neither exploration nor exploitationcan be pursued exclusively without failing at the task.\n",
    "\n",
    " The agent must try a variety ofactionsandprogressively favor those that appear to be best. \n",
    "\n",
    " On a stochastic task, eachaction must be tried many times to gain a reliable estimate of its expected reward.\n",
    "\n",
    "2. it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning takes the opposite tack, starting with a complete, interactive, goal-seeking agent.  \n",
    "\n",
    "All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When reinforcement learning involves planning, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reinforcement learning involves supervised learning, it does so for specific reasonsthat determine which capabilities are critical and which are not.\n",
    "\n",
    "For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.\n",
    "\n",
    "<!-- handler of solving a problem -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploration-exploitation trade-off\n",
    "\n",
    "The exploration-exploitation trade-off is a fundamental concept in reinforcement learning (RL).\n",
    "\n",
    "1. Exploitation:\n",
    "\n",
    "This means choosing the action that maximizes the immediate reward based on your current knowledge. Essentially, you’re exploiting what you’ve already learned.\n",
    "+ For example, if you’re playing a slot machine and you know that Machine A tends to give higher rewards, you keep pulling Machine A’s lever.\n",
    "\n",
    "2. Exploration:\n",
    "\n",
    "This means trying new actions to discover potentially better rewards in the long run.\n",
    "+ For example, even though Machine A gives good rewards, you still try Machine B or C to find out if there’s an even better option.\n",
    "\n",
    "\n",
    "Why is it important?\n",
    "+ If you only exploit, you might miss better opportunities (getting stuck in a local optimum).\n",
    "+ If you only explore, you waste time trying everything without benefiting from what you’ve already learned.\n",
    "\n",
    "3. Balancing the Trade-off:\n",
    "+ $ε$-greedy strategy: With probability $ε$, you explore randomly; otherwise, you choose the best-known action (exploit).\n",
    "+ Softmax exploration: Assigns probabilities to actions based on their expected rewards and samples accordingly.\n",
    "+ Upper Confidence Bound (UCB): Balances exploration and exploitation by considering both the reward and the uncertainty of an action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. Real-world analogy:\n",
    "\n",
    "Imagine you’re traveling in a new city.\n",
    "\t•\tExploitation: You go to the same highly-rated restaurant every day because you know it’s good.\n",
    "\t•\tExploration: You try different places to discover hidden gems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a policy,a reward signal,a value function, and, optionally, a model of the environment.\n",
    "\n",
    "+ 1. A policy defines the learning agent’s way of behaving at a given time. \n",
    "\n",
    "Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
    "\n",
    "\n",
    "+ 2. A reward signal defines the goal of a reinforcement learning problem. \n",
    "\n",
    "On each time step, the environment sends to the reinforcement learning agent a single number called the reward. \n",
    "\n",
    "The agent’s sole objective is to maximize the total reward it receives overthe long run. \n",
    "\n",
    "The reward signal thus defines what are the good and bad events for the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Whereas the reward signal indicates what is good in an immediate sense, \n",
    "+ 3. a value function specifies what is good in the long run.\n",
    "\n",
    "Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. \n",
    "\n",
    "Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states.\n",
    "\n",
    "For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reversecould be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is toachieve more reward. \n",
    "+ Nevertheless, it is values with which we are most concerned whenmaking and evaluating decisions. Action choices are made based on value judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ We seek actions that bring about states of highest value, not highest reward, \n",
    "+ because these actions obtain the greatest amount of reward for us over the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are basicallygiven directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime.\n",
    "\n",
    "In fact, the mostimportant component of almost all reinforcement learning algorithms we consider is amethod for eciently estimating values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 4. A model of the environment is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.\n",
    "\n",
    "For example, given a state and action, the model might predict the resultant next state and next reward. \n",
    "\n",
    "Models are used forplanning, by which we mean any way of decidingon a course of action by considering possible future situations before they are actuallyexperienced\n",
    "\n",
    "Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners—viewed as almost the opposite of planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state action reward value environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructsby giving correct actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Purely evaluative feedback indicateshow good the action taken was, but not whether it was the best or the worst actionpossible. \n",
    "\n",
    "+ Purely instructive feedback, on the other hand, indicates the correct action totake, independently of the action actually taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to compute Q(a)\n",
    "\n",
    "The action-value function Q(a) represents the expected cumulative reward when choosing action a in a given state and following a policy thereafter.\n",
    "\n",
    "1. In Tabular Methods (e.g., Q-Learning):\n",
    "\n",
    "If the state and action spaces are small, you can maintain a Q-table, where each entry Q(s, a) is updated iteratively based on experience.\n",
    "\n",
    "Q-Learning Update Rule:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}) - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "$s$, a: Current state and action.\n",
    "\n",
    "$r$: Immediate reward.\n",
    "\n",
    "$s{\\prime}$: Next state.\n",
    "\n",
    "$a{\\prime}$: Next action.\n",
    "\n",
    "$\\alpha$: Learning rate (how much to update).\n",
    "\n",
    "$\\gamma$: Discount factor (how much future rewards matter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros((5, 2))  # 5 states, 2 actions\n",
    "\n",
    "# Sample experience (s, a, r, s')\n",
    "state = 2\n",
    "action = 1\n",
    "reward = 10\n",
    "next_state = 3\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# Q-Learning update\n",
    "Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In Deep Q-Network (DQN):\n",
    "\n",
    "When the state space is large or continuous (e.g., images from Atari games), a neural network is used to approximate the Q-function.\n",
    "$$\n",
    "Q(s, a; \\theta) \\approx \\text{Neural Network}\n",
    "$$\n",
    "\n",
    "Input: State s.\n",
    "Output: Q-values for all possible actions.\n",
    "\n",
    "⸻\n",
    "\n",
    "DQN Loss Function:\n",
    "$$\n",
    "L(\\theta) = \\left( y_{\\text{target}} - Q(s, a; \\theta) \\right)^2\n",
    "$$\n",
    "where the target is:\n",
    "$$\n",
    "y_{\\text{target}} = r + \\gamma \\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}; \\theta^{-})\n",
    "$$\n",
    "\n",
    "$\\theta^{-}$ is the target network, a stable copy of the current Q-network.\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Exploration Strategy:\n",
    "\n",
    "To estimate Q(a) accurately, the agent needs to explore the environment. This is where strategies like ε-greedy come in, which balances exploration (trying new actions) and exploitation (choosing the best action so far)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "If you maintain estimates of the action values, then at any time step there is at leastone action whose estimated value is greatest.\n",
    "\n",
    "It refers to the action-value function Q(a), which estimates the expected reward for each action a.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\t1.\tAction-Value Estimation:\n",
    "\t•\tAs your RL agent interacts with the environment, it updates the value estimates for each action based on the rewards it receives.\n",
    "\t•\tFor example, in a simple multi-armed bandit problem (like slot machines), you estimate the average reward for each machine based on past trials.\n",
    "\t2.\tAt Every Time Step:\n",
    "\t•\tAt any point in time, there will always be one or more actions with the highest estimated value.\n",
    "\t•\tFor instance, if your estimates are Q(a_1) = 5, Q(a_2) = 8, and Q(a_3) = 3, the action a_2 is currently the best because it has the highest estimated value.\n",
    "\t3.\tHandling Ties:\n",
    "\t•\tIf multiple actions have the same highest value, you can break ties randomly or apply a small perturbation to distinguish them.\n",
    "\t4.\tWhy Is This Important?\n",
    "\t•\tIn greedy policies, you select the action with the highest estimated value (exploitation).\n",
    "\t•\tIn ε-greedy policies, you sometimes explore (choose random actions) to avoid getting stuck in local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Exploitation is the right thing to do to maximize the expected reward on the onestep, but Exploration may produce the greater total reward in the long run.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 The Problem:\n",
    "\n",
    "You just moved to a new city and want to find the best restaurant.\n",
    "+ You have three options:\n",
    "1.\tRestaurant A (you’ve been there once and had a great experience, so you believeit’s the best).\n",
    "2.\tRestaurant B (you’ve heard it’s good but haven’t tried it).\n",
    "3.\tRestaurant C (you know little about it).\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ Exploitation (Greedy Action):\n",
    "\n",
    "If you always go to Restaurant A, you are exploiting your current knowledge. This maximizes your immediate reward (a good meal), but ignores potential better options (B or C).\n",
    "\n",
    "⸻\n",
    "\n",
    "🔍 Exploration (Nongreedy Action):\n",
    "\n",
    "If you try Restaurant B or C, you are exploring. You may get a worse meal in the short term, but gather new information. If one of them is actually better than A, you can exploit it repeatedly in the future, leading to higher total rewards in the long run.\n",
    "\n",
    "⸻\n",
    "\n",
    "⚖️ The Conflict:\n",
    "+ If you always exploit, you miss better opportunities.\n",
    "+ If you only explore, you waste time on bad choices.\n",
    "\n",
    "⸻\n",
    "\n",
    "🎲 The Strategy (ε-greedy):\n",
    "+ With 90% probability, go to the best-known restaurant (exploitation).\n",
    "+ With 10% probability, try a random one (exploration).\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 In Reinforcement Learning Terms:\n",
    "+ Q(a): The estimated value of action a (like the quality of each restaurant).\n",
    "+ $\\epsilon$-greedy strategy: A balance between exploration and exploitation.\n",
    "+ Goal: Refine your action-value estimates over time and maximize long-term rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is not possible both toexplore and to exploit with any single action selection, one often refers to the “conflict”between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy action selection always exploits current knowledge tomaximize immediate reward;\n",
    "it spends no time at all sampling apparently inferior actions to see if they might really be better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why in $\\epsilon$-greedy methods, the probability of selecting the optimal action converges to greater than $1 - \\epsilon$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding $\\epsilon$-Greedy Policy\n",
    "\n",
    "In ε-greedy action selection:\n",
    " + With probability $\\epsilon$, the agent explores (chooses a random action).\n",
    " + With probability $1 - \\epsilon$, the agent exploits (chooses the best-known action).\n",
    "\n",
    "Let’s define:\n",
    " + $a^*$ as the true optimal action.\n",
    " + $\\hat{a}^*$ as the agent’s current best estimate of the optimal action (which improves over time).\n",
    "\n",
    "---\n",
    "\n",
    "2. Behavior as Learning Progresses\n",
    "\n",
    "As the agent gathers more experience:\n",
    " 1. Initially, the agent doesn’t know $a^*$, so it explores and refines $Q(s,a)$.\n",
    " 2. Over time, the Q-values converge, and the agent correctly estimates $Q(s, a^*)$.\n",
    " 3.\tEventually, $\\hat{a}^* \\to a^*$ (i.e., the agent’s best action matches the optimal action).\n",
    "\n",
    "---\n",
    "\n",
    "3. Convergence to $Probability > 1 - \\epsilon$\n",
    "\n",
    "Once the agent has correctly learned the optimal action:\n",
    " + With probability $1 - \\epsilon$, it chooses the optimal action (exploitation).\n",
    " + With probability $\\epsilon$, it chooses randomly among all actions.\n",
    "\n",
    "If there are n actions:\n",
    " + The chance of picking $a^*$ during exploration is $\\frac{1}{n}$.\n",
    " + So, the probability of picking the optimal action overall is:\n",
    "\n",
    "$$\n",
    "P(\\text{selecting } a^*) = (1 - \\epsilon) + \\epsilon \\cdot \\frac{1}{n}\n",
    "$$\n",
    "Since $\\frac{1}{n}$ is always positive, we get:\n",
    "$$\n",
    "P(\\text{selecting } a^*) > 1 - \\epsilon\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "4. Why This Matters\n",
    "\n",
    "This means that as the agent learns better Q-values, the probability of choosing the optimal action remains high (> $1 - \\epsilon$).\n",
    "\n",
    "Even with exploration, the agent still frequently picks the optimal action. This ensures that:\n",
    " + The agent exploits most of the time.\n",
    " + It still explores enough to refine Q-values.\n",
    "\n",
    " ---\n",
    "\n",
    "Example (5 Actions, $\\epsilon = 0.1$)\n",
    " + With probability 0.9, the agent picks the best-known action.\n",
    " + With probability 0.1, it randomly picks one of 5 actions ($\\frac{1}{5} = 0.2$).\n",
    " + Total probability of picking the optimal action:\n",
    "$$\n",
    "P(a^*) = 0.9 + 0.1 \\times 0.2 = 0.92\n",
    "$$\n",
    "So, the probability is greater than 1 - 0.1 = 0.9.\n",
    "\n",
    "---\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The key reason $P(a^*) > 1 - \\epsilon$ is because, even when exploring, there’s always a small chance of choosing the optimal action. Over time, as Q-values converge, the agent exploits more effectively, keeping the probability of selecting the optimal action high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1 In $\\epsilon$-greedy action selection, for the case of two actions and $\\epsilon=0.5$, what is the probability that the greedy action is selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\because$ The greedy action can be selected in two ways.\n",
    "1. With probability $1 - \\epsilon$ the agent exploit\n",
    "2. With probability $\\epsilon$ the agent explore\n",
    "\n",
    "Plus, the size of action space is $2$\n",
    "\n",
    "$ \\therefore P(\\hat{a}^8) = (1 - \\epsilon) + \\frac{1}{2} \\times \\epsilon = 0.75 $ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
