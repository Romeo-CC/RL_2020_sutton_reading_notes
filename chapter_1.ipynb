{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/imgs/Page1.jpg\" alt=\"RL basic idea\" width=\"500\" height=\"333\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Learning from interaction\n",
    "\n",
    "map situations to actions \n",
    "\n",
    "trial and error search \n",
    "\n",
    "delayed reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "...ing, like machine learning, is simultaneously a problem, a class of solution methods that work \n",
    "well on the problem, and the field studies this problem and its solution methods.\n",
    "\n",
    "It is convenient to use single name for all three things, but at the same time essential to keep\n",
    "the three conceptually separate.\n",
    "\n",
    ". In particular, the distinction between problems and solution methods is very important in \n",
    "reinforcement learning; failing tomake this distinction is the source of many confusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the real problem facing a learningagent interacting over time with its environment to achieve a goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning agent must be able to **`sense`** the state of its environment to some extent and must be able to take **`actions`** that affect the state.\n",
    "\n",
    "The agent also must have a **`goal`** or goals relating to the state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of this kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly in situations not present in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In interactive problems it is often impractical to obtain examples of desired behavior that are both correct and representative of all thesituations in which the agent has to act."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior,reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the third paradigm of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Reinforcement Learning\n",
    "\n",
    "1. trade-off between exploration and exploitation\n",
    " <!-- exploitation -->\n",
    " To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be effective in producing reward.\n",
    " <!-- exploration -->\n",
    " But to discover such actions, it has to try actions that it has not selected before.\n",
    " \n",
    " The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future.\n",
    "\n",
    " The dilemma is that neither exploration nor exploitationcan be pursued exclusively without failing at the task.\n",
    "\n",
    " The agent must try a variety ofactionsandprogressively favor those that appear to be best. \n",
    "\n",
    " On a stochastic task, eachaction must be tried many times to gain a reliable estimate of its expected reward.\n",
    "\n",
    "2. it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning takes the opposite tack, starting with a complete, interactive, goal-seeking agent.  \n",
    "\n",
    "All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When reinforcement learning involves planning, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reinforcement learning involves supervised learning, it does so for specific reasonsthat determine which capabilities are critical and which are not.\n",
    "\n",
    "For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.\n",
    "\n",
    "<!-- handler of solving a problem -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploration-exploitation trade-off\n",
    "\n",
    "The exploration-exploitation trade-off is a fundamental concept in reinforcement learning (RL).\n",
    "\n",
    "1. Exploitation:\n",
    "\n",
    "This means choosing the action that maximizes the immediate reward based on your current knowledge. Essentially, you’re exploiting what you’ve already learned.\n",
    "+ For example, if you’re playing a slot machine and you know that Machine A tends to give higher rewards, you keep pulling Machine A’s lever.\n",
    "\n",
    "2. Exploration:\n",
    "\n",
    "This means trying new actions to discover potentially better rewards in the long run.\n",
    "+ For example, even though Machine A gives good rewards, you still try Machine B or C to find out if there’s an even better option.\n",
    "\n",
    "\n",
    "Why is it important?\n",
    "+ If you only exploit, you might miss better opportunities (getting stuck in a local optimum).\n",
    "+ If you only explore, you waste time trying everything without benefiting from what you’ve already learned.\n",
    "\n",
    "3. Balancing the Trade-off:\n",
    "+ $ε$-greedy strategy: With probability $ε$, you explore randomly; otherwise, you choose the best-known action (exploit).\n",
    "+ Softmax exploration: Assigns probabilities to actions based on their expected rewards and samples accordingly.\n",
    "+ Upper Confidence Bound (UCB): Balances exploration and exploitation by considering both the reward and the uncertainty of an action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. Real-world analogy:\n",
    "\n",
    "Imagine you’re traveling in a new city.\n",
    "\t•\tExploitation: You go to the same highly-rated restaurant every day because you know it’s good.\n",
    "\t•\tExploration: You try different places to discover hidden gems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a policy,a reward signal,a value function, and, optionally, a model of the environment.\n",
    "\n",
    "+ 1. A policy defines the learning agent’s way of behaving at a given time. \n",
    "\n",
    "Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
    "\n",
    "\n",
    "+ 2. A reward signal defines the goal of a reinforcement learning problem. \n",
    "\n",
    "On each time step, the environment sends to the reinforcement learning agent a single number called the reward. \n",
    "\n",
    "The agent’s sole objective is to maximize the total reward it receives overthe long run. \n",
    "\n",
    "The reward signal thus defines what are the good and bad events for the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Whereas the reward signal indicates what is good in an immediate sense, \n",
    "+ 3. a value function specifies what is good in the long run.\n",
    "\n",
    "Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. \n",
    "\n",
    "Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states.\n",
    "\n",
    "For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reversecould be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is toachieve more reward. \n",
    "+ Nevertheless, it is values with which we are most concerned whenmaking and evaluating decisions. Action choices are made based on value judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ We seek actions that bring about states of highest value, not highest reward, \n",
    "+ because these actions obtain the greatest amount of reward for us over the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are basicallygiven directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime.\n",
    "\n",
    "In fact, the mostimportant component of almost all reinforcement learning algorithms we consider is amethod for eciently estimating values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 4. A model of the environment is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.\n",
    "\n",
    "For example, given a state and action, the model might predict the resultant next state and next reward. \n",
    "\n",
    "Models are used forplanning, by which we mean any way of decidingon a course of action by considering possible future situations before they are actuallyexperienced\n",
    "\n",
    "Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners—viewed as almost the opposite of planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state action reward value environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
